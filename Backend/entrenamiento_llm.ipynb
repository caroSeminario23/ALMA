{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carolinasv/Documents/VS_Code/ALMA/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "# Detectar la codificaci√≥n\n",
    "with open('informacion.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intentar cargar el archivo CSV con la codificaci√≥n detectada y el delimitador correcto\n",
    "try:\n",
    "    df = pd.read_csv('informacion.csv', encoding=encoding, sep=';', on_bad_lines='skip')\n",
    "    dataset1 = Dataset.from_pandas(df)\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error al parsear el archivo CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['pregunta', 'respuesta'],\n",
      "    num_rows: 85\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Leer el dataset\n",
    "print(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de filas: 85\n",
      "Numero de columnas: 2\n"
     ]
    }
   ],
   "source": [
    "# Contar el numero de filas y colunas de dataset1\n",
    "print(f\"Numero de filas: {len(dataset1)}\")\n",
    "print(f\"Numero de columnas: {len(dataset1.column_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo preentrenado de Hugging Face\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9272115230560303, 'start': 58, 'end': 70, 'answer': 'Hugging Face'}\n"
     ]
    }
   ],
   "source": [
    "# Pruebas con el modelo\n",
    "from transformers import pipeline\n",
    "\n",
    "# Cargar el pipeline de pregunta-respuesta\n",
    "nlp = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Ejemplo de texto y pregunta\n",
    "context = \"El modelo de lenguaje de Transformers es desarrollado por Hugging Face.\"\n",
    "question = \"¬øQui√©n desarroll√≥ el modelo de lenguaje de Transformers?\"\n",
    "\n",
    "# Obtener la respuesta\n",
    "result = nlp(question=question, context=context)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/85 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [00:00<00:00, 197.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizar los datos\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['pregunta'], examples['respuesta'], truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset1.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carolinasv/Documents/VS_Code/ALMA/venv/lib/python3.12/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Configurar los argumentos de entrenamiento con early stopping y weight decay\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\", # Directorio para guardar los resultados\n",
    "    evaluation_strategy=\"epoch\", # Evaluar al final de cada √©poca\n",
    "    save_strategy=\"epoch\", # Guardar al final de cada √©poca\n",
    "    learning_rate=2e-5, # Tasa de aprendizaje\n",
    "    per_device_train_batch_size=8, # Tama√±o del lote de entrenamiento por dispositivo\n",
    "    per_device_eval_batch_size=8, # Tama√±o del lote de evaluaci√≥n por dispositivo\n",
    "    num_train_epochs=3, # N√∫mero de √©pocas de entrenamiento\n",
    "    weight_decay=0.01, # Peso de la regularizaci√≥n L2\n",
    "    load_best_model_at_end=True,  # Cargar el mejor modelo al final del entrenamiento\n",
    "    metric_for_best_model=\"eval_loss\",  # M√©trica para seleccionar el mejor modelo\n",
    "    greater_is_better=False,  # Indica que una menor p√©rdida es mejor\n",
    "    save_total_limit=1,  # Limita el n√∫mero de modelos guardados\n",
    "    logging_dir='./logs',  # Directorio para los logs\n",
    "    logging_steps=10,  # Log cada 10 pasos\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Crear el objeto Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
