{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carolinasv/Documents/VS_Code/ALMA/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "# Detectar la codificación\n",
    "with open('informacion.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intentar cargar el archivo CSV con la codificación detectada y el delimitador correcto\n",
    "try:\n",
    "    df = pd.read_csv('informacion.csv', encoding=encoding, sep=';', on_bad_lines='skip')\n",
    "    dataset1 = Dataset.from_pandas(df)\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error al parsear el archivo CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['pregunta', 'respuesta'],\n",
      "    num_rows: 85\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Leer el dataset\n",
    "print(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de filas: 85\n",
      "Numero de columnas: 2\n"
     ]
    }
   ],
   "source": [
    "# Contar el numero de filas y colunas de dataset1\n",
    "print(f\"Numero de filas: {len(dataset1)}\")\n",
    "print(f\"Numero de columnas: {len(dataset1.column_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo preentrenado de Hugging Face\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud máxima permitida por el tokenizador: 1000000000000000019884624838656\n"
     ]
    }
   ],
   "source": [
    "# Obtener la longitud máxima permitida por el tokenizador\n",
    "max_length = tokenizer.model_max_length\n",
    "\n",
    "print(f\"Longitud máxima permitida por el tokenizador: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9272115230560303, 'start': 58, 'end': 70, 'answer': 'Hugging Face'}\n"
     ]
    }
   ],
   "source": [
    "# Pruebas con el modelo\n",
    "from transformers import pipeline\n",
    "\n",
    "# Cargar el pipeline de pregunta-respuesta\n",
    "nlp = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Ejemplo de texto y pregunta\n",
    "context = \"El modelo de lenguaje de Transformers es desarrollado por Hugging Face.\"\n",
    "question = \"¿Quién desarrolló el modelo de lenguaje de Transformers?\"\n",
    "\n",
    "# Obtener la respuesta\n",
    "result = nlp(question=question, context=context)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 85/85 [00:00<00:00, 143.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizar los datos\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['pregunta'], \n",
    "        examples['respuesta'], \n",
    "        truncation=True, \n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset1.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud máxima de secuencia en el dataset: 4525\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(seq) for seq in tokenized_datasets['respuesta']])\n",
    "print(f\"Longitud máxima de secuencia en el dataset: {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['pregunta', 'respuesta', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 85\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pregunta': ['¿QUÉ ES LA VIOLENCIA INTRAFAMILIAR?', '¿CON QUÉ OTROS NOMBRES ES CONOCIDA LA VIOLENCIA INTRAFAMILIAR?', '¿CUÁL ES EL OBJETIVO DE LA VIOLENCIA INTRAFAMILIAR?', '¿CUÁLES SON LOS EFECTOS DE LA VIOLENCIA INTRAFAMILIAR PARA LA VÍCTIMA?', '¿CUÁLES SON LOS EFECTOS DE LA VIOLENCIA INTRAFAMILIAR PARA LA SOCIEDAD?'], 'respuesta': ['Es el conjunto de actitudes o comportamientos de abuso (ser sexual, físico o psicológico) perpetrados en el hogar o unidad doméstica, generalmente por un miembro de la familia que vive con la víctima, que puede ser esta varón o mujer, infante, adolescente o adulto, con el empleo deliberado de la fuerza. Esto ocurre entre padres, hijos y padres y entre hermanos, por ejemplo. Además, es considera como un grave obstáculo para el desarrollo y la paz.', 'La violencia intrafamiliar también es conocida como violencia doméstica, violencia familiar o violencia en las familias.', 'El objetivo de la violencia intrafamiliar es vencer la resistencia de la víctima y obtener su subyugación, es decir, controlarla y dominarla.', 'Perjudican su integridad corporal y psicológica. Repercuten sobre su independencia e individualidad. Miedo, pánico o angustia. Heridas y lesiones físicas que, en los peores casos, pueden conducir a la muerte. Altos niveles de ansiedad por miedo a volver a ser violentados. Tristeza, desmotivación vital y depresión, Inseguridad y desconfianza absoluta ante los demás. Enfado, ira, rencor y agresividad. Bajos niveles de autoestima. Mutismo, bloqueo emocional e indefension aprendida. También, contribuye a la aparición de cánceres, enfermedades del corazón, accidentes cerebrovasculares y VIH/sida, pues las víctimas de la violencia a menudo tratan de hacer frente a sus experiencias traumáticas adoptando comportamientos de riesgo, como consumir tabaco, alcohol y drogas, así como con prácticas sexuales de riesgo. \\nSentimientos de soledad y abandono: Pueden sentirse aislados, abandonados y poco queridos. Exclusión del diálogo y la reflexión: La violencia bloquea y dificulta la capacidad para encontrar modos alternativos de resolver conflictos de forma pacífica y dialogada, Trastornos en la identidad: Pueden tener una mala imagen de sí mismos, creer que son malos y por eso sus padres los castigan físicamente. A veces, como modo de defenderse, desarrollan la creencia de que son fuertes y todopoderosos, capaces de vencer a sus padres y a otros adultos.', 'Ausentismo laboral entre las mujeres con pareja, Incremento de la delincuencia juvenil']}\n"
     ]
    }
   ],
   "source": [
    "print(dataset1[:5])  # Imprime las primeras 5 filas del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['pregunta', 'respuesta', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 85\n",
      "})\n",
      "Columnas disponibles: ['pregunta', 'respuesta', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "Tamaño del conjunto de entrenamiento: 85\n",
      "Tamaño del conjunto de validación: 85\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets)\n",
    "print(\"Columnas disponibles:\", tokenized_datasets.column_names)\n",
    "print(\"Tamaño del conjunto de entrenamiento:\", len(tokenized_datasets[\"pregunta\"]))\n",
    "print(\"Tamaño del conjunto de validación:\", len(tokenized_datasets[\"respuesta\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Crear el data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Crear los dataloaders\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"pregunta\"], batch_size=8, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"respuesta\"], batch_size=8, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Configurar los argumentos de entrenamiento con early stopping y weight decay\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\", # Directorio para guardar los resultados\n",
    "    evaluation_strategy=\"epoch\", # Evaluar al final de cada época\n",
    "    save_strategy=\"epoch\", # Guardar al final de cada época\n",
    "    learning_rate=2e-5, # Tasa de aprendizaje\n",
    "    per_device_train_batch_size=8, # Tamaño del lote de entrenamiento por dispositivo\n",
    "    per_device_eval_batch_size=8, # Tamaño del lote de evaluación por dispositivo\n",
    "    num_train_epochs=3, # Número de épocas de entrenamiento\n",
    "    weight_decay=0.01, # Peso de la regularización L2\n",
    "    load_best_model_at_end=True,  # Cargar el mejor modelo al final del entrenamiento\n",
    "    metric_for_best_model=\"eval_loss\",  # Métrica para seleccionar el mejor modelo\n",
    "    greater_is_better=False,  # Indica que una menor pérdida es mejor\n",
    "    save_total_limit=1,  # Limita el número de modelos guardados\n",
    "    logging_dir='./logs',  # Directorio para los logs\n",
    "    logging_steps=10,  # Log cada 10 pasos\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Crear el objeto Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: start_logits,end_logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/VS_Code/ALMA/venv/lib/python3.12/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/VS_Code/ALMA/venv/lib/python3.12/site-packages/transformers/trainer.py:2268\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2268\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2271\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2272\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2273\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2274\u001b[0m ):\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2276\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Documents/VS_Code/ALMA/venv/lib/python3.12/site-packages/transformers/trainer.py:3307\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3307\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3309\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3311\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Documents/VS_Code/ALMA/venv/lib/python3.12/site-packages/transformers/trainer.py:3356\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3354\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[0;32m-> 3356\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3357\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3358\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3359\u001b[0m         )\n\u001b[1;32m   3360\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   3361\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: start_logits,end_logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
